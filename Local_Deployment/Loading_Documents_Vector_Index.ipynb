{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8d33bafc",
   "metadata": {},
   "source": [
    "### Import dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d51d5b6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#import PyMuPDF\n",
    "import fitz\n",
    "\n",
    "# import llamma_index and other necessary modules\n",
    "from llama_index.core.memory import ChatMemoryBuffer\n",
    "from llama_index.core.tools import FunctionTool\n",
    "from llama_index.llms.openai import OpenAI\n",
    "from llama_index.agent.openai import OpenAIAgent\n",
    "from llama_index.core.schema import Document\n",
    "from llama_index.core.node_parser import SentenceSplitter\n",
    "from llama_index.core import SimpleDirectoryReader\n",
    "from llama_index.core.node_parser import SimpleNodeParser\n",
    "from llama_index.core.node_parser import SentenceSplitter\n",
    "from llama_index.core import SimpleDirectoryReader\n",
    "from llama_index.embeddings.fastembed import FastEmbedEmbedding\n",
    "from llama_index.core import Settings\n",
    "from llama_index.core import Document\n",
    "\n",
    "\n",
    "# import necessary modules for Qdrant and FastEmbed\n",
    "from fastembed import SparseTextEmbedding, TextEmbedding\n",
    "from qdrant_client import QdrantClient, models\n",
    "from qdrant_client.http.models import PointStruct, SparseVector\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Load environment variables\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n",
    "\n",
    "# import necessary modules for file handling and JSON processing\n",
    "from typing import List\n",
    "import pprint\n",
    "import os\n",
    "import json\n",
    "import re"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10af9469",
   "metadata": {},
   "source": [
    "### Create chunking function\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c293cd09",
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to extract text from all files in a directory\n",
    "def run_document_preprocessing(input_dir, chunk_size, chunk_overlap, output_json=\"nodes.json\"):\n",
    "    print('Processing documents....')\n",
    "    # Load documents\n",
    "    documents = SimpleDirectoryReader(input_dir=input_dir).load_data()\n",
    "    # Split documents into nodes\n",
    "    splitter = SentenceSplitter(chunk_size=chunk_size, chunk_overlap=chunk_overlap)\n",
    "    nodes = splitter.get_nodes_from_documents(documents)\n",
    "    # Convert nodes to dicts and save as JSON\n",
    "    nodes_dict = [node.to_dict() for node in nodes]\n",
    "    with open(output_json, \"w\") as f:\n",
    "        json.dump(nodes_dict, f, indent=2)\n",
    "    return {\n",
    "        \"success\": True,\n",
    "        \"message\": f\"Processed {len(nodes)} nodes. Saved to {output_json}\",\n",
    "        \"nodes_saved\": output_json,\n",
    "        \"num_nodes\": len(nodes)\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "158dd26f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to extract text from PDF files using PyMuPDF\n",
    "def extract_text_with_pymupdf(input_dir):\n",
    "    documents = []\n",
    "\n",
    "    for filename in os.listdir(input_dir):\n",
    "        if filename.endswith('.pdf'):\n",
    "            file_path = os.path.join(input_dir, filename)\n",
    "            doc = fitz.open(file_path)\n",
    "\n",
    "            for i, page in enumerate(doc):\n",
    "                text = page.get_text()\n",
    "                \n",
    "                if not text.strip():\n",
    "                    continue  # Skip blank pages\n",
    "\n",
    "                # Create one Document per page\n",
    "                document = Document(\n",
    "                    text=text,\n",
    "                    metadata={\n",
    "                        'file_path': file_path,\n",
    "                        'file_name': filename,\n",
    "                        'file_size': os.path.getsize(file_path),\n",
    "                        'file_type': 'application/pdf',\n",
    "                        'page_number': i + 1,\n",
    "                        **doc.metadata,  \n",
    "                    }\n",
    "                )\n",
    "                documents.append(document)\n",
    "\n",
    "            doc.close()\n",
    "    \n",
    "    return documents\n",
    "\n",
    "# function to extract text from all files in a directory\n",
    "def run_document_preprocessing_new(input_dir, chunk_size, chunk_overlap, output_json=\"nodes.json\"):\n",
    "    print('Processing documents....')\n",
    "    \n",
    "    documents = extract_text_with_pymupdf(input_dir)\n",
    "\n",
    "    # Split documents into nodes\n",
    "    splitter = SentenceSplitter(chunk_size=chunk_size, chunk_overlap=chunk_overlap)\n",
    "    nodes = splitter.get_nodes_from_documents(documents)\n",
    "\n",
    "    # Convert nodes to dicts and save as JSON\n",
    "    nodes_dict = [node.to_dict() for node in nodes]\n",
    "\n",
    "    with open(output_json, \"w\") as f:\n",
    "        json.dump(nodes_dict, f, indent=2)\n",
    "    return {\n",
    "        \"success\": True,\n",
    "        \"message\": f\"Processed {len(nodes)} nodes. Saved to {output_json}\",\n",
    "        \"nodes_saved\": output_json,\n",
    "        \"num_nodes\": len(nodes)\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "844966d2",
   "metadata": {},
   "source": [
    "### Create class to upload data to Qdrant"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e4ad5aa2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_31092/3003949638.py:5: UserWarning: Api key is used with an insecure connection.\n",
      "  qdrant_client = QdrantClient(\n"
     ]
    }
   ],
   "source": [
    "class Qdrant_vector_db():\n",
    "    Qdrant_API_KEY = os.getenv('QDRANT_API_KEY')\n",
    "    Qdrant_URL = os.getenv('QDRANT_URL')\n",
    "    Collection_Name = os.getenv('QDRANT_COLLECTION_NAME')\n",
    "    qdrant_client = QdrantClient(\n",
    "                                url=Qdrant_URL,\n",
    "                                api_key=Qdrant_API_KEY)\n",
    "            \n",
    "    Embeddings = {\n",
    "        \"sentence-transformer\": \"sentence-transformers/all-MiniLM-L6-v2\",\n",
    "        \"snowflake\": \"Snowflake/snowflake-arctic-embed-m\",\n",
    "        \"BAAI\": \"BAAI/bge-large-en-v1.5\",\n",
    "    }\n",
    "\n",
    "    def index_to_qdrant(self,\n",
    "        embedding_model_name: str,\n",
    "        nodes_json_path: str,\n",
    "        collection_name: str = None,\n",
    "        dense_vector_size: int = 384,\n",
    "        sparse_model_name: str = \"Qdrant/bm42-all-minilm-l6-v2-attentions\"\n",
    "    ) -> None:\n",
    "        \"\"\"\n",
    "        Index documents into Qdrant vector database with hybrid (dense + sparse) embeddings.\n",
    "        \n",
    "        Args:\n",
    "            embedding_model_name: Name of the dense embedding model (e.g., \"sentence-transformer\")\n",
    "            nodes_json_path: Path to the nodes JSON file\n",
    "            collection_name: Name of the Qdrant collection (uses env var if None)\n",
    "            dense_vector_size: Size of dense embeddings vector\n",
    "            sparse_model_name: Name of sparse embedding model\n",
    "        \"\"\"\n",
    "        # # Initialize Qdrant client\n",
    "        # qdrant_client = QdrantClient(\n",
    "        #     url=os.getenv('Qdrant_URL'),\n",
    "        #     api_key=os.getenv('Qdrant_API_KEY')\n",
    "        # )\n",
    "        \n",
    "        # Set collection name\n",
    "        collection_name = collection_name or os.getenv('collection_name')\n",
    "        \n",
    "        # Load nodes\n",
    "        print(\"Loading nodes from JSON file...\")\n",
    "        try:\n",
    "            with open(nodes_json_path, 'r') as file:\n",
    "                nodes = json.load(file)\n",
    "            documents = [node['text'] for node in nodes]\n",
    "            metadata_list = [node['metadata'] for node in nodes]\n",
    "            print(f\"Loaded {len(nodes)} nodes from {nodes_json_path}\")\n",
    "        except Exception as e:\n",
    "            print(f\"Error loading nodes: {e}\")\n",
    "            raise\n",
    "\n",
    "        # Create collection if not exists\n",
    "        if not Qdrant_vector_db.qdrant_client.collection_exists(collection_name):\n",
    "            print(f\"Creating collection '{collection_name}'...\")\n",
    "            Qdrant_vector_db.qdrant_client.create_collection(\n",
    "                collection_name=collection_name,\n",
    "                vectors_config={\n",
    "                    'dense': models.VectorParams(\n",
    "                        size=dense_vector_size,\n",
    "                        distance=models.Distance.COSINE,\n",
    "                    )\n",
    "                },\n",
    "                sparse_vectors_config={\n",
    "                    \"sparse\": models.SparseVectorParams(\n",
    "                        index=models.SparseIndexParams(on_disk=False),\n",
    "                    )\n",
    "                }\n",
    "            )\n",
    "        \n",
    "        # Initialize embedding models\n",
    "        dense_embedder = TextEmbedding(model_name=Qdrant_vector_db.Embeddings[embedding_model_name])\n",
    "        # dense_embedder = TextEmbedding(model_name=embedding_model_name)\n",
    "        sparse_embedder = SparseTextEmbedding(model_name=sparse_model_name)\n",
    "        \n",
    "        # Prepare points for upsert\n",
    "        points = []\n",
    "        for idx, (doc, metadata) in enumerate(tqdm(zip(documents, metadata_list), \n",
    "                                                total=len(documents),\n",
    "                                                desc=\"Indexing documents\")):\n",
    "            # Generate embeddings\n",
    "            dense_embedding = list(dense_embedder.embed([doc]))[0]\n",
    "            sparse_embedding = list(sparse_embedder.embed([doc]))[0]\n",
    "            \n",
    "            # Create sparse vector\n",
    "            sparse_vector = models.SparseVector(\n",
    "                indices=sparse_embedding.indices.tolist(),\n",
    "                values=sparse_embedding.values.tolist()\n",
    "            )\n",
    "            \n",
    "            # Create point structure\n",
    "            points.append(models.PointStruct(\n",
    "                id=idx,\n",
    "                vector={\n",
    "                    \"dense\": dense_embedding.tolist(),\n",
    "                    \"sparse\": sparse_vector\n",
    "                },\n",
    "                payload={\n",
    "                    \"text\": doc,\n",
    "                    **metadata\n",
    "                }\n",
    "            ))\n",
    "        \n",
    "        # Upsert points\n",
    "        Qdrant_vector_db.qdrant_client.upsert(\n",
    "            collection_name=collection_name,\n",
    "            points=points\n",
    "        )\n",
    "        print(f\"Successfully indexed {len(points)} documents in collection '{collection_name}'\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc4395d1",
   "metadata": {},
   "source": [
    "### Process documents"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ec314b9",
   "metadata": {},
   "source": [
    "#### Extract text and create chunks in JSON format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "532c4d24",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing documents....\n",
      "{'message': 'Processed 176 nodes. Saved to nodes.json',\n",
      " 'nodes_saved': 'nodes.json',\n",
      " 'num_nodes': 176,\n",
      " 'success': True}\n"
     ]
    }
   ],
   "source": [
    "# assign the result of funciton to variable\n",
    "result = run_document_preprocessing_new(\n",
    "    input_dir=\"./Data\",\n",
    "    chunk_size=500,\n",
    "    chunk_overlap=100,\n",
    "    output_json=\"nodes.json\"\n",
    "    )\n",
    "\n",
    "# print variable\n",
    "pprint.pprint(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "420b7623",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create an instance of Qdrant_vector_db class\n",
    "qdrant_db=Qdrant_vector_db()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4c48a898",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading nodes from JSON file...\n",
      "Loaded 176 nodes from nodes.json\n",
      "Creating collection 'finance_documents'...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Indexing documents: 100%|██████████| 176/176 [00:23<00:00,  7.34it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully indexed 176 documents in collection 'finance_documents'\n"
     ]
    }
   ],
   "source": [
    "# run method to index documents into qdrant\n",
    "qdrant_db.index_to_qdrant(\n",
    "        embedding_model_name=\"sentence-transformer\",\n",
    "        nodes_json_path=\"nodes.json\",\n",
    "        collection_name=\"finance_documents\",\n",
    "        dense_vector_size=384,\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa5823d1",
   "metadata": {},
   "source": [
    "# *** ONLY RUN THE NOTEBOOK UP TO THIS POINT ***\n",
    "\n",
    "From this point on the notebook explains the most critical parts of the code"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8aaeafe",
   "metadata": {},
   "source": [
    "### Code analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac5e1c9f",
   "metadata": {},
   "source": [
    "#### Text loading and splitting - existing version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02dfe893",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_dir = \"./Data\"\n",
    "chunk_size = 500\n",
    "chunk_overlap = 100\n",
    "\n",
    "# Load documents\n",
    "documents = SimpleDirectoryReader(input_dir=input_dir).load_data()\n",
    "\n",
    "# Split documents into nodes\n",
    "splitter = SentenceSplitter(chunk_size=chunk_size, chunk_overlap=chunk_overlap)\n",
    "nodes = splitter.get_nodes_from_documents(documents)\n",
    "\n",
    "# Convert nodes to dicts and save as JSON\n",
    "nodes_dict_ex = [node.to_dict() for node in nodes]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a9efccf",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(nodes_dict_ex)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "241a8ba5",
   "metadata": {},
   "outputs": [],
   "source": [
    "n = 20\n",
    "nodes_dict_ex[n]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae9265a1",
   "metadata": {},
   "source": [
    "#### Text loading and splitting - improved version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fafef38d",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_dir = \"./Data\"\n",
    "chunk_size = 500\n",
    "chunk_overlap = 100\n",
    "\n",
    "import fitz  # PyMuPDF\n",
    "from llama_index.core import Document\n",
    "import os\n",
    "\n",
    "def extract_text_with_pymupdf(input_dir):\n",
    "    documents = []\n",
    "    \n",
    "    for filename in os.listdir(input_dir):\n",
    "        if filename.endswith('.pdf'):\n",
    "            file_path = os.path.join(input_dir, filename)\n",
    "            \n",
    "            # Open PDF with PyMuPDF\n",
    "            doc = fitz.open(file_path)\n",
    "            text = \"\"\n",
    "            \n",
    "            for page in doc:\n",
    "                text += page.get_text()\n",
    "            \n",
    "            doc.close()\n",
    "            \n",
    "            # Create LlamaIndex Document\n",
    "            document = Document(\n",
    "                text=text,\n",
    "                metadata={\n",
    "                    'file_path': file_path,\n",
    "                    'file_name': filename,\n",
    "                    'file_type': 'application/pdf'\n",
    "                }\n",
    "            )\n",
    "            documents.append(document)\n",
    "    \n",
    "    return documents\n",
    "\n",
    "documents = extract_text_with_pymupdf(input_dir)\n",
    "\n",
    "# Split documents into nodes\n",
    "splitter = SentenceSplitter(chunk_size=chunk_size, chunk_overlap=chunk_overlap)\n",
    "nodes = splitter.get_nodes_from_documents(documents)\n",
    "\n",
    "# Convert nodes to dicts and save as JSON\n",
    "nodes_dict = [node.to_dict() for node in nodes]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a654f2c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(nodes_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68056076",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the first node as an example\n",
    "nodes_dict[n]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91911a5b",
   "metadata": {},
   "source": [
    "Comments:\n",
    "\n",
    "- The improved version of the text extractor using PyMuPDF extracts readable text\n",
    "- The number of vectors are reduced significantly, from 1,462 a 82\n",
    "- The dense and the sparse vectors run smoother into the Qdrant vector store"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1d94f9b",
   "metadata": {},
   "source": [
    "#### Vectore store\n",
    "\n",
    "- Requires a Qdrant database instance running in http:localhost:6333\n",
    "- The Qdrant database instance can be easily setup in a Docker container. This requires Docker available in the machine where the chatbot will be deployed\n",
    "- The files in this repository have all required instructions to install Python dependencies and launch the Docker Qdrant instance"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01da779d",
   "metadata": {},
   "source": [
    "##### Dense and sparse vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f712b1e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a Qdrant client instance\n",
    "Qdrant_API_KEY = os.getenv('QDRANT_API_KEY')\n",
    "Qdrant_URL = os.getenv('QDRANT_URL')\n",
    "Collection_Name = os.getenv('QDRANT_COLLECTION_NAME')\n",
    "qdrant_client = QdrantClient(\n",
    "                            url=Qdrant_URL,\n",
    "                            api_key=Qdrant_API_KEY)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f82543a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set variables\n",
    "Embeddings = {\n",
    "        \"sentence-transformer\": \"sentence-transformers/all-MiniLM-L6-v2\",\n",
    "        \"snowflake\": \"Snowflake/snowflake-arctic-embed-m\",\n",
    "        \"BAAI\": \"BAAI/bge-large-en-v1.5\",\n",
    "    }\n",
    "\n",
    "embedding_model_name=\"BAAI\"\n",
    "nodes_json_path=\"nodes.json\"\n",
    "collection_name=\"finance_documents\"\n",
    "dense_vector_size=1024\n",
    "sparse_model_name: str = \"Qdrant/bm42-all-minilm-l6-v2-attentions\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4cb1f31",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load nodes\n",
    "print(\"Loading nodes from JSON file...\")\n",
    "try:\n",
    "    with open(nodes_json_path, 'r') as file:\n",
    "        nodes = json.load(file)\n",
    "    documents = [node['text'] for node in nodes]\n",
    "    metadata_list = [node['metadata'] for node in nodes]\n",
    "    print(f\"Loaded {len(nodes)} nodes from {nodes_json_path}\")\n",
    "except Exception as e:\n",
    "    print(f\"Error loading nodes: {e}\")\n",
    "    raise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d0f8704",
   "metadata": {},
   "outputs": [],
   "source": [
    "#explore docuemnts\n",
    "documents[n]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6976b57f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# explore metadata\n",
    "metadata_list[n]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec32c5d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create collection if not exists\n",
    "if not Qdrant_vector_db.qdrant_client.collection_exists(collection_name):\n",
    "    print(f\"Creating collection '{collection_name}'...\")\n",
    "    Qdrant_vector_db.qdrant_client.create_collection(\n",
    "        collection_name=collection_name,\n",
    "        vectors_config={\n",
    "            'dense': models.VectorParams(\n",
    "                size=dense_vector_size,\n",
    "                distance=models.Distance.COSINE,\n",
    "            )\n",
    "        },\n",
    "        sparse_vectors_config={\n",
    "            \"sparse\": models.SparseVectorParams(\n",
    "                index=models.SparseIndexParams(on_disk=False),\n",
    "            )\n",
    "        }\n",
    "    )\n",
    "    print(f\"Collection '{collection_name}' created successfully.\")\n",
    "else:\n",
    "    print(f\"Collection '{collection_name}' already exists.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53444c28",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize embedding models\n",
    "!export TOKENIZERS_PARALLELISM=false"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test the API connection (should require your API key)\n",
    "!curl -H \"api-key: $QDRANT_API_KEY\" http://localhost:6333/collections"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e592cd5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# check points in the collection\n",
    "!curl -H \"api-key: $QDRANT_API_KEY\" http://localhost:6333/collections/finance_documents/points\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62162d0e",
   "metadata": {},
   "source": [
    "Comments:\n",
    "- The collection was created\n",
    "- The collection is empty"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7e674b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize embedding models\n",
    "dense_embedder = TextEmbedding(model_name=Qdrant_vector_db.Embeddings[embedding_model_name])\n",
    "sparse_embedder = SparseTextEmbedding(model_name=sparse_model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# understand the structure of zip()\n",
    "for idx, (doc, metadata) in enumerate(zip(documents, metadata_list)):\n",
    "    if idx < 50:\n",
    "        print(f\"Index: {idx}\")\n",
    "        print(f\"Document: {doc[:50]}...\")  # Print first 50 characters\n",
    "        print(f\"Metadata: {metadata}\")\n",
    "        print(\"-\" * 80)\n",
    "    else:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d655edb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# understand dense vector\n",
    "dense_embedding = list(dense_embedder.embed([documents[0]]))[0]\n",
    "print(f\"Dense Embedding: {dense_embedding[:10]}...\")  # Print first"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "760d0af3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# understand sparse vector\n",
    "sparse_embedding = list(sparse_embedder.embed([documents[0]]))[0]\n",
    "print(f\"Sparse Embedding Indices: {sparse_embedding.indices[:10]}...\")\n",
    "print(f\"Sparse Embedding Values: {sparse_embedding.values[:10]}...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1be9af6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare points for upsert\n",
    "points = []\n",
    "for idx, (doc, metadata) in enumerate(tqdm(zip(documents, metadata_list), \n",
    "                                        total=len(documents),\n",
    "                                        desc=\"Indexing documents\")):\n",
    "    # Generate embeddings\n",
    "    dense_embedding = list(dense_embedder.embed([doc]))[0]\n",
    "    sparse_embedding = list(sparse_embedder.embed([doc]))[0]\n",
    "    \n",
    "    # Create sparse vector\n",
    "    sparse_vector = models.SparseVector(\n",
    "        indices=sparse_embedding.indices.tolist(),\n",
    "        values=sparse_embedding.values.tolist()\n",
    "    )\n",
    "    \n",
    "    # Create point structure\n",
    "    points.append(models.PointStruct(\n",
    "        id=idx,\n",
    "        vector={\n",
    "            \"dense\": dense_embedding.tolist(),\n",
    "            \"sparse\": sparse_vector\n",
    "        },\n",
    "        payload={\n",
    "            \"text\": doc,\n",
    "            **metadata\n",
    "        }\n",
    "    ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8688bd1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# explore points\n",
    "len(points)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "726b6334",
   "metadata": {},
   "source": [
    "Comments:\n",
    "- There are as many points as there are text chunks\n",
    "- Each point follows the PointStruct from Ddrant http.models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c94a9c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# explore the first point\n",
    "points[n]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6260f095",
   "metadata": {},
   "outputs": [],
   "source": [
    "points[n].vector['dense'][:10]  # First 10 elements of dense vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d55ad4bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(points[n].vector['dense'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "points[n].payload['text']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ff5fd93",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92ddf3db",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83973775",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1976eb6",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
